{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf42d238-1745-4bd3-98d5-d6d5f4b9075b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "error",
     "timestamp": 1733954486930,
     "user": {
      "displayName": "Cathy V",
      "userId": "00379111358150357952"
     },
     "user_tz": -60
    },
    "id": "cbf135fe-7b03-4cf9-b41b-0d2a5218a2a3",
    "outputId": "2c65b234-1fad-48ef-9bfe-decdcbf18f82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO       The following preferences have been changed for Brian2GeNN, reset them manually if you use a different device later in the same script: codegen.loop_invariant_optimisations, core.network.default_schedule [brian2.devices.genn]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch n.0\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Multiple clocks are not supported for the genn device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 543\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (_stimulus_indexes, _output_stimuli, _input_stimuli) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl):\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch n.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 543\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnum_pause_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\AI 2024-25\\Programming_utils\\envs\\neural_computation_env\\Lib\\site-packages\\brian2\\core\\base.py:333\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    331\u001b[0m curdev \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(curdev, name):\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\AI 2024-25\\Programming_utils\\envs\\neural_computation_env\\Lib\\site-packages\\brian2genn\\device.py:1898\u001b[0m, in \u001b[0;36mGeNNDevice.network_run\u001b[1;34m(self, net, duration, report, report_period, namespace, profile, level, **kwds)\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m   1897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (obj\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefaultclock\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (CodeRunner, StateMonitor)):\n\u001b[1;32m-> 1898\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1899\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiple clocks are not supported for the genn device\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m   1902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_linked_variables\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Multiple clocks are not supported for the genn device"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import brian2 as b2\n",
    "from brian2 import *\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset_util import Synthetic_Dataset_Utils\n",
    "\n",
    "\n",
    "import brian2genn\n",
    "\n",
    "msec_step = 100\n",
    "shared_clock = Clock(dt=msec_step*ms)\n",
    "\n",
    "\n",
    "set_device('genn', use_GPU=True)\n",
    "\n",
    "\n",
    "# TODO: Make weights always non-negative! (maybe not necessary)\n",
    "# Equations for everything. I_post in the synapse is modifying I in the target neuron\n",
    "\n",
    "\n",
    "# Parameters for neurons\n",
    "Cm = 281 * pF  # Membrane capacitance\n",
    "gL = 30 * nS   # Leak conductance\n",
    "EL = -70.6 * mV  # Leak reversal potential\n",
    "Vth = -50.4 * mV  # Spike threshold\n",
    "\n",
    "DeltaT = 2 * mV  # Slope factor\n",
    "Vr = -70.6 * mV  # Reset potential\n",
    "\n",
    "Vcut = -40 * mV  # Cutoff potential for spike generation\n",
    "tau_A = 1 * ms  # Adaptation time constant\n",
    "c = 4 * nS       # Coupling parameter\n",
    "b = 0.0805 * nA  # Spike-triggered adaptation increment\n",
    "\n",
    "# Parameters for synapses\n",
    "tau_rise = 5 * ms  # Rise time constant for AMPA\n",
    "tau_decay = 50 * ms  # Decay time constant for NMDA\n",
    "w_init = 0.5 * nS   # Initial synaptic weight (conductance)\n",
    "\n",
    "# AdEx neuron equations\n",
    "eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + Iexc + Iinh - A ) / Cm : volt \n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp \n",
    "dX/dt = -X / tau_decay + Y / tau_rise : volt   # Spike trace\n",
    "dY/dt = -Y / tau_decay : volt  # Glutamate decay \n",
    "Iexc : amp  \n",
    "Iinh : amp \n",
    "batch_sum_X : 1   # Accumulate X values over a batch\n",
    "running_sum_X : 1  # Sum of X during a stimulus\n",
    "'''\n",
    "# Ii = I + input_stimuli(t)  :  amp  # Input current\n",
    "\n",
    "\n",
    "input_eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt \n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp \n",
    "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
    "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
    "I = input_stimuli(t, indices) : amp \n",
    "indices : integer \n",
    "'''\n",
    "\n",
    "out_eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I_tot - A ) / Cm : volt \n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
    "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
    "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
    "Iexc : amp  # Synaptic input \n",
    "Iexc2 : amp  # Synaptic input \n",
    "I_tot = Iexc + Iexc2 + output_stimuli(t, indices) : amp \n",
    "indices : integer  # dimensionless index variable\n",
    "'''\n",
    "\n",
    "# Synapse equations (Spike trace dynamics)\n",
    "\n",
    "\n",
    "syn_eqs_exc = '''\n",
    "w : siemens   # Synaptic weight, conductance\n",
    "Iexc_post = w * X_pre : amp (summed)\n",
    "'''\n",
    "\n",
    "syn_eqs_exc_output = '''\n",
    "w : siemens   # Synaptic weight, conductance\n",
    "Iexc2_post = w * X_pre : amp (summed)\n",
    "'''\n",
    "\n",
    "syn_eqs_inh= '''\n",
    "w : siemens   # Synaptic weight, conductance\n",
    "Iinh_post = w * X_pre : amp (summed)\n",
    "'''\n",
    "\n",
    "def update_batch_sum(synapses):\n",
    "    synapses.batch_sum_X += synapses.X  # Perform the operation\n",
    "\n",
    "# dims = [784,400,225,64]\n",
    "dims = [50,40,30,20]\n",
    "\n",
    "gist_dim = 16\n",
    "\n",
    "# Just a function to make neurongroups of specified dimension. Assigns input equations (which read the input timedarray, hopefully) \n",
    "# to the first group if with_input = True\n",
    "def make_groups(dims, eqs = eqs, with_input = False):\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for i, dim in enumerate(dims):\n",
    "    \n",
    "        if i == 0 and with_input == True:\n",
    "    \n",
    "            groups.append(NeuronGroup(dim, input_eqs, threshold='v > Vcut', reset='v = Vr', method='euler', clock=shared_clock))\n",
    "    \n",
    "        else:\n",
    "    \n",
    "            groups.append(NeuronGroup(dim, eqs, threshold='v > Vcut', reset='v = Vr', method='euler', clock=shared_clock))\n",
    "    \n",
    "    for group in groups:\n",
    "        \n",
    "        group.v = EL\n",
    "    \n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "def make_bottom_up_connections(R, E_0, E_1, syn_eqs_exc, syn_eqs_inh):\n",
    "\n",
    "\n",
    "  S_p = Synapses(R, E_0, model=syn_eqs_exc,\n",
    "        on_pre='Y_post = 1*volt ', clock=shared_clock)\n",
    "\n",
    "    \n",
    "  S_p.connect(condition='i == j')  # One-to-one connections\n",
    "  S_p.w = 'w_init'\n",
    "\n",
    "\n",
    "  S_m = Synapses(R, E_1, model=syn_eqs_inh,\n",
    "        on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "  S_m.connect(condition='i == j')  # One-to-one connections\n",
    "  S_m.w = 'w_init'\n",
    "\n",
    "  return S_p, S_m\n",
    "\n",
    "def make_top_down_connections(R, E_0, E_1, syn_eqs_exc, syn_eqs_inh):\n",
    "\n",
    "\n",
    "  S_p = Synapses(R, E_1, model=syn_eqs_exc,\n",
    "        on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "  S_p.connect()\n",
    "  S_p.w = 'rand() * w_init'\n",
    "\n",
    "\n",
    "  S_m = Synapses(R, E_0, model=syn_eqs_inh,\n",
    "        on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "  S_m.connect()\n",
    "  S_m.w = 'rand() * w_init'\n",
    "\n",
    "\n",
    "  return S_p, S_m\n",
    "\n",
    "def make_gist_connections(Rs, G):\n",
    "\n",
    "  S = Synapses(Rs[0], G, model=syn_eqs_exc,\n",
    "        on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "  S.connect(p=0.05)  # Connect with 5% probability\n",
    "  S.w = 'rand() * w_init'   # This is slightly different from the paper: it should be based on a ratio\n",
    "\n",
    "  S_gist_input = S\n",
    "\n",
    "  for R in Rs[1:]:\n",
    "    S = Synapses(G, R, model=syn_eqs_exc,\n",
    "        on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "    S.connect(p=0.05)  # Connect with 5% probability\n",
    "    S_gist_output = S\n",
    "\n",
    "  return S_gist_input, S_gist_output\n",
    "\n",
    "# num_classes_per_layer and max_depth depend on the same parameters used when generating the datset\n",
    "\n",
    "def make_output_layer(num_classes_per_layer, max_depth, w_out_init = None ):\n",
    "\n",
    "    if w_out_init == None:\n",
    "        w_out_init = w_init/10\n",
    "        \n",
    "        \n",
    "\n",
    "    total_neurons = sum([num_classes_per_layer**(d+1) for d in range(max_depth)])\n",
    "    \n",
    "    \n",
    "    Os = NeuronGroup(N=total_neurons, model = out_eqs, threshold='v > Vcut', reset='v = Vr', method='euler', clock=shared_clock)\n",
    "\n",
    "    \n",
    "\n",
    "    S_o = Synapses(Os, Os, model=syn_eqs_exc_output, on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "    \n",
    "    trace = 0\n",
    "    for d in range(num_classes_per_layer - 1):\n",
    "\n",
    "        new_trace = trace + num_classes_per_layer**(d+1)\n",
    "        \n",
    "        source_indices = list(range(trace, trace + num_classes_per_layer**(d+1)))\n",
    "        target_indices = list(range(new_trace , new_trace + num_classes_per_layer**(d+2)))\n",
    "\n",
    "        ii, jj = np.meshgrid(source_indices, target_indices, indexing='ij')\n",
    "        \n",
    "        S_o.connect(i=ii.flatten(), j=jj.flatten())\n",
    "\n",
    "        trace = new_trace\n",
    "\n",
    "\n",
    "    stimulus_indices = []\n",
    "\n",
    "    for i, n in enumerate([num_classes_per_layer**i for i in range(1, max_depth+1)]):\n",
    "        for _ in range(n):\n",
    "            stimulus_indices.append(int(i))\n",
    "            \n",
    "    Os.indices = stimulus_indices \n",
    "\n",
    "\n",
    "    return Os, S_o\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_network(dims, syn_eqs_exc, syn_eqs_inh, num_classes_per_layer, max_depth, w_out_init = None ):\n",
    "\n",
    "    max_depth += 1\n",
    "    \n",
    "    Rs = make_groups(dims, with_input = True)\n",
    "    Es_0 = make_groups(dims[:-1])\n",
    "    Es_1 = make_groups(dims[:-1])\n",
    "    G = NeuronGroup(gist_dim, eqs, threshold='v > Vcut', reset='v = Vr', method='euler', clock=shared_clock)\n",
    "    \n",
    "    Os, S_o_internal = make_output_layer(num_classes_per_layer, max_depth) # Hopefully this is correct\n",
    "    \n",
    "    connections = {}\n",
    "    connections[\"output_internal\"] = S_o_internal\n",
    "    \n",
    "    for i in range(len(Rs)-1):\n",
    "    \n",
    "    \n",
    "        S_p, S_m = make_bottom_up_connections(Rs[i], Es_0[i], Es_1[i], syn_eqs_exc, syn_eqs_inh)\n",
    "        \n",
    "        connections[f\"bottom_up_{i}\"] = [S_p, S_m]\n",
    "    \n",
    "        if i != len(Rs)-1:\n",
    "        \n",
    "            S_p, S_m = make_top_down_connections(Rs[i+1], Es_0[i], Es_1[i], syn_eqs_exc, syn_eqs_inh)\n",
    "            \n",
    "            connections[f\"top_down_{i}\"] = [S_p, S_m]\n",
    "        \n",
    "    S_gist_input, S_gist_output = make_gist_connections(Rs, G)\n",
    "    \n",
    "    connections[\"gist_input\"] = S_gist_input\n",
    "    connections[\"gist_output\"] = S_gist_output\n",
    "    \n",
    "    '''\n",
    "    S_o_external = Synapses(Rs[-1], Os, model=syn_eqs_exc,\n",
    "        on_pre='Y_post = 1*volt')\n",
    "    S_o_external.connect()\n",
    "    S_o_external.w = 'rand() * w_init'\n",
    "    '''\n",
    "    S_o_external = []\n",
    "    for O in Os:\n",
    "        S = Synapses(Rs[-1], O, model=syn_eqs_exc, on_pre='Y_post = 1*volt', clock=shared_clock)\n",
    "        S.connect()\n",
    "        S.w = 'rand() * w_init'\n",
    "        S_o_external.append(S)\n",
    "    connections[\"output_external\"] = S_o_external\n",
    "\n",
    "\n",
    "    return Rs, Es_0, Es_1, G, S_gist_input, S_gist_output, Os, S_o_external, connections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helpful for setting up input\n",
    "\n",
    "def normalize_tensor(tensor, old_min, old_max, new_min, new_max):\n",
    "\n",
    "    normalized_tensor = (tensor - old_min) / (old_max - old_min)\n",
    "\n",
    "\n",
    "    scaled_tensor = normalized_tensor * (new_max - new_min) + new_min\n",
    "\n",
    "    return scaled_tensor\n",
    "\n",
    "\n",
    "\n",
    "def normalize_and_unwrap_dataset(dt, minval,maxval):\n",
    "\n",
    "\n",
    "    minimum = 0\n",
    "    maximum = 0\n",
    "    \n",
    "    all_curves = [torch.tensor(curve, dtype = torch.float64) for curve in chain.from_iterable(dt[\"curves\"].values())]\n",
    "    \n",
    "    for curve in all_curves:\n",
    "    \n",
    "        _min = torch.min(curve)\n",
    "        _max = torch.max(curve)\n",
    "        if(minimum > _min): minimum = _min\n",
    "        if(maximum < _max): maximum = _max\n",
    "\n",
    "\n",
    "    unwrapped = []\n",
    "    for key in dt[\"categories\"].keys():\n",
    "        for curve in dt[\"curves\"][key]:\n",
    "            unwrapped.append((torch.tensor(key), normalize_tensor(torch.tensor(curve), minimum, maximum, minval, maxval)))\n",
    "            \n",
    "    return unwrapped\n",
    "            \n",
    "\n",
    "def get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer):\n",
    "\n",
    "    if labels.ndim == 1:\n",
    "        labels = labels.unsqueeze(0)  # Convert to 2D with shape (1, len(I_indexes))\n",
    "\n",
    "    indexes = torch.zeros_like(labels)\n",
    "    \n",
    "    for i in range(0, labels.shape[1]):\n",
    "    \n",
    "        summed = 0\n",
    "        if i>0:\n",
    "            summed = torch.stack([labels[:,j]*num_classes_per_layer**(i-j) for j in range(i)]).sum(dim=0)\n",
    "    \n",
    "        indexes[:,i] = summed + labels[:,i] # These are more intelligible, since they indicate which neuron to stimulate for each level of granularity\n",
    "\n",
    "    # But we need to adapt them for a situation where all neurons are concatenated in a single list\n",
    "    \n",
    "    to_sum = 0\n",
    "    for i in range(0, labels.shape[1]):\n",
    "\n",
    "        if i>0:\n",
    "            to_sum += num_classes_per_layer**i \n",
    "\n",
    "        indexes[:,i] = indexes[:,i] + to_sum\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def get_output_current_arrays(I_indexes, dim, I_value):\n",
    "    # Ensure I_indexes is 2D for consistent processing\n",
    "    if I_indexes.ndim == 1:\n",
    "        I_indexes = I_indexes.unsqueeze(0)  # Convert to 2D with shape (1, len(I_indexes))\n",
    "\n",
    "    # Create an output tensor of zeros\n",
    "    out = torch.zeros((I_indexes.shape[0], dim), dtype=torch.float32)\n",
    "\n",
    "    # Row indices (batch indices) for advanced indexing\n",
    "    row_indices = torch.arange(I_indexes.shape[0]).repeat_interleave(I_indexes.shape[1])\n",
    "\n",
    "    # Flattened column indices (curve indexes)\n",
    "    col_indices = I_indexes.flatten()\n",
    "\n",
    "    # Set the values using advanced indexing\n",
    "    out[row_indices, col_indices] = I_value\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class CurveDataset(Dataset):\n",
    "    def __init__(self, data, minval, maxval):\n",
    "        \n",
    "        self.data = normalize_and_unwrap_dataset(data, minval, maxval)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        labels, curves = self.data[idx]\n",
    "      \n",
    "  \n",
    "        return labels, curves.flatten()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch, msec_step, num_pause_blocks, num_classes_per_layer, out_dim):\n",
    "\n",
    "    \n",
    "\n",
    "    labels, curves = zip(*batch)\n",
    "\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    curves = torch.stack(curves, dim=0)\n",
    "\n",
    "\n",
    "   \n",
    "    pause = torch.zeros_like(curves[0])  \n",
    "    \n",
    "    pause_block = torch.tile(pause, (num_pause_blocks, 1))  \n",
    "    \n",
    "    # Interleave stimulus rows with the pause block\n",
    "    curves_with_pause = torch.vstack([torch.vstack((row, pause_block)) for row in curves])\n",
    "\n",
    "    visual_stimulus = TimedArray(curves_with_pause.numpy() * nA, dt=msec_step*ms)\n",
    "\n",
    "\n",
    "    stimulus_indexes = TimedArray(np.tile([1] +\n",
    "                                     [0] * num_pause_blocks, curves.size(0)), dt=msec_step*ms)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    out_stimuli_idx = get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer)\n",
    "   \n",
    "    out_stimuli_array = get_output_current_arrays(out_stimuli_idx, out_dim, 1)\n",
    "\n",
    "    label_pause = torch.zeros(out_dim)  \n",
    "    \n",
    "    label_pause_block = torch.tile(label_pause, (num_pause_blocks, 1)) \n",
    "\n",
    "    out_stimuli_with_pause = torch.vstack([torch.vstack((row, label_pause_block)) for row in out_stimuli_array])\n",
    "\n",
    "    output_stimuli = TimedArray(out_stimuli_with_pause.numpy() * nA, dt=msec_step*ms) \n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "    return stimulus_indexes, output_stimuli , visual_stimulus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dims = [2, 2,2,2]\n",
    "Rs, Es_0, Es_1, G, S_gist_input, S_gist_output, Os, S_o_external, connections = make_network(dims, syn_eqs_exc, syn_eqs_inh, num_classes_per_layer=3, max_depth=3)\n",
    "\n",
    "Rs[0].indices = range(Rs[0].N)\n",
    "\n",
    "su = Synthetic_Dataset_Utils()\n",
    "\n",
    "ranges = [30,30,30,30]  # Ranges for each parameter\n",
    "\n",
    "prior_params = [10,10,10,10]  # Initial parameters\n",
    "\n",
    "num_samples_per_class=5\n",
    "N=15\n",
    "\n",
    "# Build the tree\n",
    "max_depth = 3 # Adjust as needed\n",
    "num_classes_per_layer = 3  # Adjust as needed\n",
    "\n",
    "std_multiplier = 1\n",
    "\n",
    "tree = su.build_tree(prior_params, 0, max_depth, num_classes_per_layer, std_multiplier, ranges)\n",
    "\n",
    "synth_dataset = su.make_dataset(tree, num_samples_per_class=num_samples_per_class, N=N)\n",
    "\n",
    "output_dim = sum([len(o) for o in Os])\n",
    "\n",
    "lr = 0.03\n",
    "msec_step = 500\n",
    "batch_size = 20\n",
    "num_pause_blocks = 1\n",
    "\n",
    "dataset = CurveDataset(synth_dataset, 0.6, 1.5)\n",
    "dl = DataLoader( dataset, shuffle = True, batch_size = batch_size, collate_fn = partial(collate_fn, msec_step=msec_step, num_pause_blocks=num_pause_blocks,\n",
    "                                                                                num_classes_per_layer = num_classes_per_layer, out_dim = output_dim) )\n",
    "\n",
    "# Update logic\n",
    "@network_operation( clock=shared_clock)\n",
    "def add_inputs():\n",
    "    stimulus_indexes = _stimulus_indexes \n",
    "    output_stimuli = _output_stimuli\n",
    "    input_stimuli = _input_stimuli\n",
    "\n",
    "@network_operation(dt = msec_step*ms)\n",
    "def debug():\n",
    "    print(\"Rs[0].I: \", Rs[0].I[:])\n",
    "   \n",
    "@network_operation( clock=shared_clock)\n",
    "def update_sums():\n",
    "\n",
    "    current_idx = int(defaultclock.t / msec_step*ms)\n",
    "    \n",
    "    if stimulus_indexes.values[current_idx] == 1:\n",
    "\n",
    "        \n",
    "        for n in range(len(Rs)-1):\n",
    "\n",
    "            if n > 0:\n",
    "                Rs[n+1].batch_sum_X += Rs[n+1].X_\n",
    "                \n",
    "            Es_0[n].batch_sum_X += Es_0[n].X_\n",
    "            Es_1[n].batch_sum_X += Es_1[n].X_\n",
    "\n",
    "            \n",
    "@network_operation(clock=shared_clock)\n",
    "def apply_weight_update():\n",
    "    \n",
    "    print(\"Applying weight update\")\n",
    "    \n",
    "    for n in range(len(Rs)-1):\n",
    "\n",
    "        print(\"n: \", n)\n",
    "        \n",
    "        for i in range(len(Rs[n+1])):\n",
    "            for j in range(len(Es_0[n])):\n",
    "   \n",
    "            \n",
    "                S_p, S_m = connections[f\"top_down_{n}\"]\n",
    "    \n",
    "                S_p.w[i,j] += lr * Es_0[n].batch_sum_X[j] * Rs[n+1].batch_sum_X[i] * siemens\n",
    "                \n",
    "                S_m.w[i,j] += lr * Es_1[n].batch_sum_X[j] * Rs[n+1].batch_sum_X[i]  * siemens\n",
    "            \n",
    "                Rs[n+1].batch_sum_X[i] = 0\n",
    "                \n",
    "                Es_0[n].batch_sum_X[i]= 0\n",
    "    \n",
    "                Es_1[n].batch_sum_X[i] = 0\n",
    "\n",
    "\n",
    "stimulus_indexes, output_stimuli, input_stimuli = next(iter(dl))\n",
    "\n",
    "monitor = StateMonitor(Rs[0], ['v', 'I'], record=[0])\n",
    "spike_monitor = SpikeMonitor(Rs[0])\n",
    "\n",
    "net = Network(Rs, Es_0, Es_1, G,  Os, connections, monitor, spike_monitor, add_inputs, debug, update_sums, apply_weight_update)\n",
    "\n",
    "for i, (_stimulus_indexes, _output_stimuli, _input_stimuli) in enumerate(dl):\n",
    "\n",
    "\n",
    "    print(f\"batch n.{i}\")\n",
    "\n",
    "\n",
    "\n",
    "    net.run((batch_size + batch_size*num_pause_blocks) * 100 * ms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c791d-797d-4b45-9c58-9891429fba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778742e-37c3-439d-afb7-da728be51fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639ea6b-c9fa-4019-adef-734c4899d160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1dcc06-81d5-499d-9fce-eb8d7a9d089e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485c9ac-195e-4221-a629-75af4422fcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e49ef9-29d6-40be-a0b0-defbe2f36826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5af8d-ad06-45e8-a0fe-5bd678943b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7de56-20b5-4998-a35a-86e86f301bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(vars(spike_monitor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d5551-26cc-4093-88ac-28068d6dec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spike_monitor = SpikeMonitor(Os)\n",
    "\n",
    "\n",
    "# Plot membrane potential\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "\n",
    "\n",
    "# Firing rate plot\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Spike Count')\n",
    "plt.title('Firing Rate of Neuron 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zkD5Zs72axi9",
   "metadata": {
    "id": "zkD5Zs72axi9"
   },
   "outputs": [],
   "source": [
    "def training_phase_1(network, dl):\n",
    "\n",
    "  for label_input, curve_input in dl:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lOfMXzfIaxdn",
   "metadata": {
    "id": "lOfMXzfIaxdn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T9V1Ufi3axau",
   "metadata": {
    "id": "T9V1Ufi3axau"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Membrane potential plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(monitor.t / ms, monitor.v[0] / mV, label='mV')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.title('Input')\n",
    "plt.legend()\n",
    "\n",
    "# Firing rate plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Spike Count')\n",
    "plt.title('Input')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jJZrHvJVaxXE",
   "metadata": {
    "id": "jJZrHvJVaxXE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ee7d2-a787-41b0-a184-154cbb8d3242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f51cc-b549-4b0c-83f2-eb6df4bfaf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2226e1-6834-4dff-82e4-59e522ee9a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fb598-329c-41f4-8998-97d02000a5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d563b-4185-43e6-b26a-41ae6cfdd1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "586f1b19-bab1-4990-b2ca-1aabe4ae053c",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc4819-5071-4651-9378-4a9ab4117d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(\n",
    "    *Rs,        # Unpack the list of NeuronGroups\n",
    "    *Es_0,      # Unpack the list of NeuronGroups\n",
    "    *Es_1,      # Unpack the list of NeuronGroups\n",
    "    G,          # Single NeuronGroup is fine\n",
    "    Os,         # Single NeuronGroup is fine\n",
    "    connections[\"output_internal\"],   # Single Synapses object is fine\n",
    "    connections[\"gist_input\"],        # Single Synapses object is fine\n",
    "    *connections[\"gist_output\"],      # Unpack the list of Synapses\n",
    "    *connections[\"output_external\"],  # Unpack the list of Synapses\n",
    "    spike_monitor, \n",
    "    monitor,\n",
    "    debug, \n",
    "    update_sums, \n",
    "    apply_weight_update\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JUtGtvxAaxRO",
   "metadata": {
    "id": "JUtGtvxAaxRO"
   },
   "outputs": [],
   "source": [
    "eqs_neuron = '''\n",
    "dv/dt = (-gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A) / Cm : volt\n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
    "I = stimulus(t)  : amp  \n",
    "'''\n",
    "\n",
    "\n",
    "neuron = make_groups([1], eqs = eqs_neuron)[0]\n",
    "\n",
    "neuron\n",
    "\n",
    "# Change 2 to any value from 0.6 to 1.5, which is the range of current intensity we are probably going to use\n",
    "\n",
    "stimulus = TimedArray(np.hstack([[c] for c in np.ones(1)*0.8]) * nA, dt=10*ms)\n",
    "\n",
    "\n",
    "\n",
    "neuron.v = EL\n",
    "neuron.A = 0 * nA\n",
    "\n",
    "# Monitor the specific neuron (neuron 2)\n",
    "monitor = StateMonitor(neuron, ['v', 'I'], record=[0])\n",
    "spike_monitor = SpikeMonitor(neuron)\n",
    "\n",
    "run(1 * second)\n",
    "\n",
    "# Plot membrane potential\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Membrane potential plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(monitor.t / ms, monitor.v[0] / mV, label='Neuron 2 Membrane Potential')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Membrane Potential (mV)')\n",
    "plt.title('Stimulus Applied to Single Neuron')\n",
    "plt.legend()\n",
    "\n",
    "# Firing rate plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Spike Count')\n",
    "plt.title('Firing Rate of Neuron 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "0.6: 0\n",
    "0.625: 0\n",
    "0.63: 0.5\n",
    "0.65: 1.5\n",
    "0.7: 2.8\n",
    "0.8: 4.5\n",
    "1: 7.5\n",
    "1.5: 14\n",
    "2: 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f18219-77eb-4144-8b49-d2df646ad21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's connect 2 groups as a trial\n",
    "\n",
    "G1 = NeuronGroup(5, 'v : volt', threshold='v > Vcut', reset='v = Vr', method='euler')  # Presynaptic neurons\n",
    "G2 = NeuronGroup(5, eqs, threshold='v > Vcut', reset='v = Vr; A += b', method='euler')  # Postsynaptic neurons\n",
    "\n",
    "# Initialize variables\n",
    "G1.v = EL\n",
    "G2.v = EL\n",
    "G2.A = 0 * nA\n",
    "\n",
    "# Create synapses\n",
    "S = Synapses(G1, G2, model=syn_eqs_exc,\n",
    "             on_pre='Y_post = 1*volt', method = \"euler\")  # Increment glutamate release on spike\n",
    "S.connect(p=0.1)  # Random connections\n",
    "S.w = 'rand() * w_init'  # Random initial weights\n",
    "\n",
    "# Monitors\n",
    "spike_mon_G1 = SpikeMonitor(G1)\n",
    "spike_mon_G2 = SpikeMonitor(G2)\n",
    "state_mon_G2 = StateMonitor(G2, ['v', 'I', 'A'], record=True)\n",
    "\n",
    "# Run simulation\n",
    "b2.run(500 * ms)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot membrane potential of a postsynaptic neuron\n",
    "plt.subplot(311)\n",
    "plt.plot(state_mon_G2.t / ms, state_mon_G2.v[0] / mV, label='Membrane potential (v)')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33487b0-f5ed-4b97-9956-8a57570ef79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make weights always non-negative!\n",
    "\n",
    "# Parameters for neurons\n",
    "Cm = 281 * pF  # Membrane capacitance\n",
    "gL = 30 * nS   # Leak conductance\n",
    "EL = -70.6 * mV  # Leak reversal potential\n",
    "Vth = -50.4 * mV  # Spike threshold\n",
    "DeltaT = 2 * mV  # Slope factor\n",
    "Vr = -70.6 * mV  # Reset potential\n",
    "Vcut = -40 * mV  # Cutoff potential for spike generation\n",
    "tau_A = 1 * ms  # Adaptation time constant\n",
    "c = 4 * nS       # Coupling parameter\n",
    "b = 0.0805 * nA  # Spike-triggered adaptation increment\n",
    "\n",
    "# Parameters for synapses\n",
    "tau_rise = 5 * ms  # Rise time constant for AMPA\n",
    "tau_decay = 50 * ms  # Decay time constant for NMDA\n",
    "w_init = 0.5 * nS   # Initial synaptic weight (conductance)\n",
    "\n",
    "# AdEx neuron equations\n",
    "eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
    "I : amp  # Input current\n",
    "'''\n",
    "\n",
    "input_eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
    "I = input_stimuli(t)  : amp  \n",
    "'''\n",
    "\n",
    "out_eqs = '''\n",
    "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I_tot - A ) / Cm : volt\n",
    "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
    "I : amp  # Synaptic input (summed)\n",
    "I_tot = I + output_stimuli(t, indices) : amp\n",
    "indices : integer  # dimensionless index variable\n",
    "'''\n",
    "\n",
    "# Synapse equations (Spike trace dynamics)\n",
    "\n",
    "\n",
    "syn_eqs_exc = '''\n",
    "dX/dt = -X / tau_decay + Y / tau_rise : volt   # Spike trace\n",
    "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
    "w : siemens  # Synaptic weight (conductance)\n",
    "I_post = w * X : amp (summed)\n",
    "batch_sum_X : volt  # Accumulate X values over a batch\n",
    "running_sum_X : volt  # Sum of X during a stimulus\n",
    "'''\n",
    "\n",
    "syn_eqs_inh= '''\n",
    "dX/dt = -X / tau_decay + Y / tau_rise : volt  # Spike trace\n",
    "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
    "w : siemens  # Synaptic weight (conductance)\n",
    "I_post = w * X : amp (summed)\n",
    "batch_sum_X : volt  # Accumulate X values over a batch\n",
    "running_sum_X : volt  # Sum of X during a stimulus\n",
    "'''\n",
    "\n",
    "def update_batch_sum(synapses):\n",
    "    synapses.batch_sum_X += synapses.X  # Perform the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f595c7b-4896-4d7e-8dfb-3590cd46a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logic\n",
    "@network_operation(dt = batch_size * msec_step * ms)\n",
    "def update_sums( ):\n",
    "\n",
    "    current_idx = int(defaultclock.t / msec_step*ms)\n",
    "    \n",
    "    if stimulus_indexes.values[current_idx] == 1:\n",
    "\n",
    "        \n",
    "        for n in range(len(Rs)-1):\n",
    "\n",
    "            for i in range(len(Rs[n])):\n",
    " \n",
    "                '''\n",
    "                Rs[n].batch_sum_X[i] += Rs[n].X_[i]\n",
    "                \n",
    "                Es_0[n].batch_sum_X[i] += Es_0[n].X_[i]\n",
    "    \n",
    "                Es_1[n].batch_sum_X[i] += Es_1[n].X_[i]\n",
    "                '''\n",
    "\n",
    "                Rs[n].batch_sum_X += Rs[n].X_\n",
    "                Es_0[n].batch_sum_X += Es_0[n].X_\n",
    "                Es_1[n].batch_sum_X += Es_1[n].X_\n",
    "        \n",
    "            \n",
    "@network_operation( dt = batch_size * msec_step * ms )\n",
    "def apply_weight_update():\n",
    "    \n",
    "    print(\"Applying weight update\")\n",
    "    \n",
    "    for n in range(len(Rs)-1):\n",
    "        print(\"n: \", n)\n",
    "        \n",
    "        for i in range(len(Rs[n])):\n",
    "            print(\"i: \", i)\n",
    "            for j in range(len(Es_0[n])):\n",
    "                print(\"j: \", j)\n",
    "    \n",
    "            \n",
    "                S_p, S_m = connections[f\"top_down_{n}\"]\n",
    "    \n",
    "                S_p.w[i,j] += lr * Es_0[n].batch_sum_X[j] * Rs[n].batch_sum_X[i] * siemens\n",
    "                \n",
    "                S_m.w[i,j] += lr * Es_1[n].batch_sum_X[j] * Rs[n].batch_sum_X[i]  * siemens\n",
    "            \n",
    "                Rs[n].batch_sum_X[i] = 0\n",
    "                \n",
    "                Es_0[n].batch_sum_X[i]= 0\n",
    "    \n",
    "                Es_1[n].batch_sum_X[i] = 0\n",
    "\n",
    "\n",
    "\n",
    "for stimulus_indexes, output_stimuli, input_stimuli in dl:\n",
    "\n",
    "    print(\"indexes\", stimulus_indexes.values.shape, stimulus_indexes.values)\n",
    "\n",
    "    run((batch_size + batch_size*num_pause_blocks) * 100 * ms)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural_computation_kernel",
   "language": "python",
   "name": "neural_computation_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
