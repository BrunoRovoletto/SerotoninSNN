{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf42d238-1745-4bd3-98d5-d6d5f4b9075b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf42d238-1745-4bd3-98d5-d6d5f4b9075b",
        "outputId": "01c9f9aa-50cd-44c1-b8b9-2a57061d12af",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING    'i' is an internal variable of group 'synapses_17', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
            "WARNING    'i' is an internal variable of group 'synapses_18', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
            "WARNING    'i' is an internal variable of group 'synapses_21', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
            "WARNING    'i' is an internal variable of group 'synapses_22', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
            "WARNING    'i' is an internal variable of group 'synapses_25', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
            "WARNING    'i' is an internal variable of group 'synapses_26', but also exists in the run namespace with the value 2. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import brian2 as b2\n",
        "from brian2 import *\n",
        "from itertools import chain\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from dataset_util import Synthetic_Dataset_Utils\n",
        "\n",
        "\n",
        "import brian2cuda\n",
        "\n",
        "from brian2 import prefs\n",
        "from brian2 import set_device\n",
        "\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "set_device(\"cpp_standalone\", build_on_run=False)\n",
        "\n",
        "#set_device(\"cuda_standalone\", build_on_run=True)\n",
        "# Set specific preferences for CUDA standalone\n",
        "prefs.devices.cuda_standalone.profile_statemonitor_copy_to_host = None\n",
        "prefs.devices.cuda_standalone.cuda_backend.extra_compile_args_nvcc = ['-use_fast_math', '-w']\n",
        "prefs.devices.cuda_standalone.cuda_backend.detect_gpus = True\n",
        "prefs.devices.cuda_standalone.cuda_backend.gpu_id = 0  # Use the first GPU\n",
        "###############################################################################\n",
        "# Model Parameters and Equations\n",
        "###############################################################################\n",
        "msec_step = 100 * ms\n",
        "\n",
        "# TODO: Make weights always non-negative!\n",
        "\n",
        "\n",
        "# Equations for everything. I_post in the synapse is modifying I in the target neuron\n",
        "\n",
        "\n",
        "# TODO: Make weights always non-negative! (maybe not necessary)\n",
        "# Equations for everything. I_post in the synapse is modifying I in the target neuron\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parameters for neurons\n",
        "Cm = 281 * pF  # Membrane capacitance\n",
        "gL = 30 * nS   # Leak conductance\n",
        "EL = -70.6 * mV  # Leak reversal potential\n",
        "Vth = -50.4 * mV  # Spike threshold\n",
        "\n",
        "DeltaT = 2 * mV  # Slope factor\n",
        "Vr = -70.6 * mV  # Reset potential\n",
        "\n",
        "Vcut = -40 * mV  # Cutoff potential for spike generation\n",
        "tau_A = 1 * ms  # Adaptation time constant\n",
        "c = 4 * nS       # Coupling parameter\n",
        "b = 0.0805 * nA  # Spike-triggered adaptation increment\n",
        "\n",
        "# Parameters for synapses\n",
        "tau_rise = 5 * ms  # Rise time constant for AMPA\n",
        "tau_decay = 50 * ms  # Decay time constant for NMDA\n",
        "w_init = 0.5 * nS   # Initial synaptic weight (conductance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# AdEx neuron equations\n",
        "eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + Iexc + Iinh - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt   # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "Iexc : amp\n",
        "Iinh : amp\n",
        "batch_sum_X : 1   # Accumulate X values over a batch\n",
        "running_sum_X : 1  # Sum of X during a stimulus\n",
        "'''\n",
        "# Ii = I + input_stimuli(t)  :  amp  # Input current\n",
        "\n",
        "\n",
        "input_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "I = input_stimuli(t, indices) : amp\n",
        "indices : integer\n",
        "'''\n",
        "\n",
        "out_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I_tot - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "Iexc : amp  # Synaptic input\n",
        "Iexc2 : amp  # Synaptic input\n",
        "I_tot = Iexc + Iexc2 + output_stimuli(t, indices) : amp\n",
        "indices : integer  # dimensionless index variable\n",
        "spike_count = 0 : integer\n",
        "'''\n",
        "\n",
        "# Synapse equations (Spike trace dynamics)\n",
        "\n",
        "\n",
        "syn_eqs_exc = '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iexc_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "syn_eqs_exc_output = '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iexc2_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "syn_eqs_inh= '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iinh_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "def update_batch_sum(synapses):\n",
        "    synapses.batch_sum_X += synapses.X  # Perform the operation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Helper Functions\n",
        "###############################################################################\n",
        "def make_groups(dims, eqs=eqs, with_input=False):\n",
        "    groups = []\n",
        "    for i, dim in enumerate(dims):\n",
        "        if i == 0 and with_input:\n",
        "            g = NeuronGroup(dim, input_eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "        else:\n",
        "            g = NeuronGroup(dim, eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "        g.v = EL\n",
        "        groups.append(g)\n",
        "    return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_bottom_up_connections(R, E_0, E_1):\n",
        "    S_p = Synapses(R, E_0, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_p.connect(condition='i == j')\n",
        "    S_p.w = 'w_init'\n",
        "\n",
        "    S_m = Synapses(R, E_1, model=syn_eqs_inh, on_pre='Y_post = 1*volt')\n",
        "    S_m.connect(condition='i == j')\n",
        "    S_m.w = 'w_init'\n",
        "\n",
        "    return S_p, S_m\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_top_down_connections(R, E_0, E_1):\n",
        "    S_p = Synapses(R, E_1, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_p.connect()\n",
        "    S_p.w = 'rand() * w_init'\n",
        "\n",
        "    S_m = Synapses(R, E_0, model=syn_eqs_inh, on_pre='Y_post = 1*volt')\n",
        "    S_m.connect()\n",
        "    S_m.w = 'rand() * w_init'\n",
        "\n",
        "    return S_p, S_m\n",
        "\n",
        "def make_gist_connections(Rs, G):\n",
        "    S_gist_input = Synapses(Rs[0], G, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_gist_input.connect(p=0.05)\n",
        "    S_gist_input.w = 'rand() * w_init'\n",
        "\n",
        "    # Connect G to all other layers\n",
        "    S_gist_output = Synapses(G, Rs[-1], model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_gist_output.connect(p=0.05)\n",
        "    S_gist_output.w = 'rand() * w_init'\n",
        "    return S_gist_input, S_gist_output\n",
        "\n",
        "def make_output_layer(num_classes_per_layer, max_depth, w_out_init=None):\n",
        "    if w_out_init is None:\n",
        "        w_out_init = w_init/10\n",
        "\n",
        "    total_neurons = sum([num_classes_per_layer**(d+1) for d in range(max_depth)])\n",
        "    Os = NeuronGroup(N=total_neurons, model=out_eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "\n",
        "    S_o = Synapses(Os, Os, model=syn_eqs_exc_output, on_pre='Y_post = 1*volt')\n",
        "\n",
        "    trace = 0\n",
        "    for d in range(num_classes_per_layer - 1):\n",
        "        new_trace = trace + num_classes_per_layer**(d+1)\n",
        "        source_indices = list(range(trace, trace + num_classes_per_layer**(d+1)))\n",
        "        target_indices = list(range(new_trace, new_trace + num_classes_per_layer**(d+2)))\n",
        "        ii, jj = np.meshgrid(source_indices, target_indices, indexing='ij')\n",
        "        S_o.connect(i=ii.flatten(), j=jj.flatten())\n",
        "        trace = new_trace\n",
        "\n",
        "    # Assign indices for output stimuli\n",
        "    stimulus_indices = []\n",
        "    for i in range(max_depth):\n",
        "        for _ in range(num_classes_per_layer**(i+1)):\n",
        "            stimulus_indices.append(i)\n",
        "    Os.indices = stimulus_indices\n",
        "\n",
        "    return Os, S_o\n",
        "\n",
        "def make_network(dims, num_classes_per_layer, max_depth, w_out_init=None):\n",
        "    max_depth += 1\n",
        "    Rs = make_groups(dims, with_input=True)\n",
        "    Es_0 = make_groups(dims[:-1])\n",
        "    Es_1 = make_groups(dims[:-1])\n",
        "    G = NeuronGroup(16, eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "\n",
        "    Os, S_o_internal = make_output_layer(num_classes_per_layer, max_depth, w_out_init=w_out_init)\n",
        "\n",
        "    connections = {}\n",
        "    for i in range(len(Rs)-1):\n",
        "        S_p, S_m = make_bottom_up_connections(Rs[i], Es_0[i], Es_1[i])\n",
        "        connections[f\"bottom_up_{i}\"] = [S_p, S_m]\n",
        "\n",
        "        if i != len(Rs)-1:\n",
        "            S_p_td, S_m_td = make_top_down_connections(Rs[i+1], Es_0[i], Es_1[i])\n",
        "            connections[f\"top_down_{i}\"] = [S_p_td, S_m_td]\n",
        "\n",
        "    S_gist_input, S_gist_output = make_gist_connections(Rs, G)\n",
        "    connections[\"gist_input\"] = [S_gist_input]\n",
        "    connections[\"gist_output\"] = [S_gist_output]\n",
        "\n",
        "    # External output connections\n",
        "    # In original code: attempted to connect Rs[-1] to Os\n",
        "    # We'll do a single large synapse group for simplicity:\n",
        "    S_o_external = Synapses(Rs[-1], Os, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_o_external.connect()\n",
        "    S_o_external.w = 'rand() * w_init'\n",
        "    connections[\"output_external\"] = [S_o_external]\n",
        "\n",
        "    return Rs, Es_0, Es_1, G, Os, S_o_internal, connections\n",
        "\n",
        "def normalize_tensor(tensor, old_min, old_max, new_min, new_max):\n",
        "    normalized_tensor = (tensor - old_min) / (old_max - old_min)\n",
        "    scaled_tensor = normalized_tensor * (new_max - new_min) + new_min\n",
        "    return scaled_tensor\n",
        "\n",
        "def normalize_and_unwrap_dataset(dt, minval, maxval):\n",
        "    minimum = 0\n",
        "    maximum = 0\n",
        "    all_curves = [torch.tensor(curve, dtype=torch.float64) for curve in chain.from_iterable(dt[\"curves\"].values())]\n",
        "    for curve in all_curves:\n",
        "        _min = torch.min(curve)\n",
        "        _max = torch.max(curve)\n",
        "        if (minimum > _min): minimum = _min\n",
        "        if (maximum < _max): maximum = _max\n",
        "\n",
        "    unwrapped = []\n",
        "    for key in dt[\"categories\"].keys():\n",
        "        for curve in dt[\"curves\"][key]:\n",
        "            unwrapped.append((torch.tensor(key), normalize_tensor(torch.tensor(curve), minimum, maximum, minval, maxval)))\n",
        "    return unwrapped\n",
        "\n",
        "def get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer):\n",
        "    if labels.ndim == 1:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    indexes = torch.zeros_like(labels)\n",
        "    for i in range(labels.shape[1]):\n",
        "        summed = 0\n",
        "        if i > 0:\n",
        "            summed = torch.stack([labels[:, j]*num_classes_per_layer**(i-j) for j in range(i)]).sum(dim=0)\n",
        "        indexes[:, i] = summed + labels[:, i]\n",
        "\n",
        "    # Adjust indexes for concatenation\n",
        "    to_sum = 0\n",
        "    for i in range(labels.shape[1]):\n",
        "        if i > 0:\n",
        "            to_sum += num_classes_per_layer**i\n",
        "        indexes[:, i] = indexes[:, i] + to_sum\n",
        "    return indexes\n",
        "\n",
        "def get_output_current_arrays(I_indexes, dim, I_value):\n",
        "    if I_indexes.ndim == 1:\n",
        "        I_indexes = I_indexes.unsqueeze(0)\n",
        "\n",
        "    out = torch.zeros((I_indexes.shape[0], dim), dtype=torch.float32)\n",
        "    row_indices = torch.arange(I_indexes.shape[0]).repeat_interleave(I_indexes.shape[1])\n",
        "    col_indices = I_indexes.flatten()\n",
        "    out[row_indices, col_indices] = I_value\n",
        "    return out\n",
        "\n",
        "class CurveDataset(Dataset):\n",
        "    def __init__(self, data, minval, maxval):\n",
        "        self.data = normalize_and_unwrap_dataset(data, minval, maxval)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        labels, curves = self.data[idx]\n",
        "        return labels, curves.flatten()\n",
        "\n",
        "def collate_fn(batch, msec_step, num_pause_blocks, num_classes_per_layer, out_dim):\n",
        "    labels, curves = zip(*batch)\n",
        "    labels = torch.stack(labels, dim=0)\n",
        "    curves = torch.stack(curves, dim=0)\n",
        "\n",
        "    pause = torch.zeros_like(curves[0])\n",
        "    pause_block = torch.tile(pause, (num_pause_blocks, 1))\n",
        "    curves_with_pause = torch.vstack([torch.vstack((row, pause_block)) for row in curves])\n",
        "    visual_stimulus = TimedArray(curves_with_pause.numpy() * nA, dt=msec_step)\n",
        "\n",
        "    stimulus_indexes = TimedArray(np.tile([1] + [0] * num_pause_blocks, curves.size(0)), dt=msec_step)\n",
        "\n",
        "    out_stimuli_idx = get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer)\n",
        "    out_stimuli_array = get_output_current_arrays(out_stimuli_idx, out_dim, 1)\n",
        "    label_pause = torch.zeros(out_dim)\n",
        "    label_pause_block = torch.tile(label_pause, (num_pause_blocks, 1))\n",
        "    out_stimuli_with_pause = torch.vstack([torch.vstack((row, label_pause_block)) for row in out_stimuli_array])\n",
        "    output_stimuli = TimedArray(out_stimuli_with_pause.numpy() * nA, dt=msec_step)\n",
        "\n",
        "    return stimulus_indexes, output_stimuli, visual_stimulus\n",
        "\n",
        "###############################################################################\n",
        "# Build the network and run\n",
        "###############################################################################\n",
        "dims = [2, 2, 2, 2]\n",
        "num_classes_per_layer = 3\n",
        "max_depth = 3\n",
        "\n",
        "Rs, Es_0, Es_1, G, Os, S_o_internal, connections = make_network(dims, num_classes_per_layer, max_depth)\n",
        "\n",
        "Rs[0].indices = range(Rs[0].N)\n",
        "\n",
        "su = Synthetic_Dataset_Utils()\n",
        "ranges = [30, 30, 30, 30]\n",
        "prior_params = [10, 10, 10, 10]\n",
        "num_samples_per_class = 5\n",
        "N = 15\n",
        "tree = su.build_tree(prior_params, 0, max_depth, num_classes_per_layer, 1, ranges)\n",
        "synth_dataset = su.make_dataset(tree, num_samples_per_class=num_samples_per_class, N=N)\n",
        "\n",
        "output_dim = Os.N\n",
        "lr = 0.03\n",
        "batch_size = 100\n",
        "num_pause_blocks = 1\n",
        "\n",
        "dataset = CurveDataset(synth_dataset, 0.6, 1.5)\n",
        "dl = DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=partial(\n",
        "        collate_fn,\n",
        "        msec_step=msec_step,\n",
        "        num_pause_blocks=num_pause_blocks,\n",
        "        num_classes_per_layer=num_classes_per_layer,\n",
        "        out_dim=output_dim\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "@network_operation()\n",
        "def debug():\n",
        "    print(\"Rs[0].I: \", Rs[0].I[:])\n",
        "\n",
        "@network_operation()\n",
        "def update_sums():\n",
        "    current_idx = int(defaultclock.t / msec_step)\n",
        "    if stimulus_indexes.values[current_idx] == 1:\n",
        "        for n in range(len(Rs)-1):\n",
        "            if n > 0:\n",
        "                Rs[n+1].batch_sum_X += Rs[n+1].X\n",
        "            Es_0[n].batch_sum_X += Es_0[n].X\n",
        "            Es_1[n].batch_sum_X += Es_1[n].X\n",
        "\n",
        "@network_operation()\n",
        "def apply_weight_update():\n",
        "    print(\"Applying weight update\")\n",
        "    for n in range(len(Rs)-1):\n",
        "        \n",
        "        print(\"n: \", n)\n",
        "        \n",
        "        # Retrieve top_down synapses       \n",
        "        S_p, S_m = connections[f\"top_down_{n}\"]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "            \n",
        "        for i_idx in range(Es_0[n].N):\n",
        "            for j_idx in range(Rs[n+1].N):\n",
        "                \n",
        "                S_p.w[i_idx, j_idx] += lr * Es_0[n].batch_sum_X[i_idx] * Rs[n+1].batch_sum_X[j_idx] * siemens\n",
        "                S_m.w[i_idx, j_idx] += lr * Es_1[n].batch_sum_X[i_idx] * Rs[n+1].batch_sum_X[j_idx] * siemens\n",
        "                \n",
        "     \n",
        "            # Reset after update\n",
        "            Rs[n+1].batch_sum_X[:] = 0\n",
        "            Es_0[n].batch_sum_X[:] = 0\n",
        "            Es_1[n].batch_sum_X[:] = 0\n",
        "    \n",
        "    \n",
        "    \n",
        "    external_syn = connections[\"output_external\"][0]\n",
        "    \n",
        "    for i_idx in range(Rs[-1].N):\n",
        "        for j_idx in range(Os.N):\n",
        "            \n",
        "            external_syn.w[i_idx, j_idx] += lr * Rs[-1].batch_sum_X[i_idx] * Os.batch_sum_X[j_idx] * siemens\n",
        "            \n",
        "            \n",
        "    Rs[-1].batch_sum_X[:] = 0\n",
        "    Os.batch_sum_X[:] = 0\n",
        "        \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "@network_operation()\n",
        "def log_spikes():\n",
        "    # Everything here runs on CPU\n",
        "    # Get the new spikes from spike_mon\n",
        "    # For example, we could retrieve the current length or the last spike index\n",
        "    # Then write them to a file with Python I/O\n",
        "    print(\"CHUNGUS IS BIG\")\n",
        "    new_spike_indices = spike_mon.i[:]\n",
        "    new_spike_times   = spike_mon.t[:]\n",
        "    # For demonstration, write them out\n",
        "    with open(\"/content/outputs.txt\", \"a\") as f:\n",
        "        for i_spk, t_spk in zip(new_spike_indices, new_spike_times):\n",
        "            f.write(f\"{t_spk} {i_spk}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Extract all synapses and objects for the network\n",
        "all_synapses = []\n",
        "all_synapses.append(S_o_internal)\n",
        "\n",
        "for i in range(len(Rs)-1):\n",
        "    bu_syns = connections[f\"bottom_up_{i}\"]\n",
        "    all_synapses.extend(bu_syns)\n",
        "    td_key = f\"top_down_{i}\"\n",
        "    if td_key in connections:\n",
        "        td_syns = connections[td_key]\n",
        "        all_synapses.extend(td_syns)\n",
        "\n",
        "gist_input_syn = connections[\"gist_input\"][0]\n",
        "gist_output_syn = connections[\"gist_output\"][0]\n",
        "all_synapses.append(gist_input_syn)\n",
        "all_synapses.append(gist_output_syn)\n",
        "\n",
        "external_syn = connections[\"output_external\"][0]\n",
        "all_synapses.append(external_syn)\n",
        "\n",
        "monitor = StateMonitor(Rs[0], ['v', 'I'], record=[0])\n",
        "spike_monitor = SpikeMonitor(Rs[0])\n",
        "\n",
        "s = 0\n",
        "\n",
        "\n",
        "\n",
        "spike_mon = SpikeMonitor(Os)\n",
        "\n",
        "\n",
        "net = Network(\n",
        "    Rs, Es_0, Es_1, G, Os,\n",
        "    all_synapses,\n",
        "    monitor, spike_monitor,\n",
        "    debug, update_sums, apply_weight_update, log_spikes\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "all_batch_rates = []\n",
        "output_spikes = []\n",
        "\n",
        "s = 0\n",
        "stimulus_indexes, output_stimuli, input_stimuli = next(iter(dl))\n",
        "\n",
        "\n",
        "\n",
        "run_time = (batch_size + batch_size*num_pause_blocks)*100*ms\n",
        "net.run(run_time)\n",
        "net.stop()\n",
        "\n",
        "device.build()\n",
        "\n",
        "# device.build(directory='cuda_project', compile=True, run=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e795c43",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff221b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import brian2 as b2\n",
        "from brian2 import *\n",
        "from itertools import chain\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from dataset_util import Synthetic_Dataset_Utils\n",
        "\n",
        "\n",
        "import brian2cuda\n",
        "\n",
        "from brian2 import prefs\n",
        "from brian2 import set_device\n",
        "\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "set_device(\"cpp_standalone\", build_on_run=False)\n",
        "\n",
        "#set_device(\"cuda_standalone\", build_on_run=True)\n",
        "# Set specific preferences for CUDA standalone\n",
        "prefs.devices.cuda_standalone.profile_statemonitor_copy_to_host = None\n",
        "prefs.devices.cuda_standalone.cuda_backend.extra_compile_args_nvcc = ['-use_fast_math', '-w']\n",
        "prefs.devices.cuda_standalone.cuda_backend.detect_gpus = True\n",
        "prefs.devices.cuda_standalone.cuda_backend.gpu_id = 0  # Use the first GPU\n",
        "###############################################################################\n",
        "# Model Parameters and Equations\n",
        "###############################################################################\n",
        "msec_step = 100 * ms\n",
        "\n",
        "# TODO: Make weights always non-negative!\n",
        "\n",
        "\n",
        "# Equations for everything. I_post in the synapse is modifying I in the target neuron\n",
        "\n",
        "\n",
        "# TODO: Make weights always non-negative! (maybe not necessary)\n",
        "# Equations for everything. I_post in the synapse is modifying I in the target neuron\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parameters for neurons\n",
        "Cm = 281 * pF  # Membrane capacitance\n",
        "gL = 30 * nS   # Leak conductance\n",
        "EL = -70.6 * mV  # Leak reversal potential\n",
        "Vth = -50.4 * mV  # Spike threshold\n",
        "\n",
        "DeltaT = 2 * mV  # Slope factor\n",
        "Vr = -70.6 * mV  # Reset potential\n",
        "\n",
        "Vcut = -40 * mV  # Cutoff potential for spike generation\n",
        "tau_A = 1 * ms  # Adaptation time constant\n",
        "c = 4 * nS       # Coupling parameter\n",
        "b = 0.0805 * nA  # Spike-triggered adaptation increment\n",
        "\n",
        "# Parameters for synapses\n",
        "tau_rise = 5 * ms  # Rise time constant for AMPA\n",
        "tau_decay = 50 * ms  # Decay time constant for NMDA\n",
        "w_init = 0.5 * nS   # Initial synaptic weight (conductance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# AdEx neuron equations\n",
        "eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + Iexc + Iinh - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt   # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "Iexc : amp\n",
        "Iinh : amp\n",
        "batch_sum_X : 1   # Accumulate X values over a batch\n",
        "running_sum_X : 1  # Sum of X during a stimulus\n",
        "'''\n",
        "# Ii = I + input_stimuli(t)  :  amp  # Input current\n",
        "\n",
        "\n",
        "input_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "I = input_stimuli(t, indices) : amp\n",
        "indices : integer\n",
        "'''\n",
        "\n",
        "out_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I_tot - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt    # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "Iexc : amp  # Synaptic input\n",
        "Iexc2 : amp  # Synaptic input\n",
        "I_tot = Iexc + Iexc2 + output_stimuli(t, indices) : amp\n",
        "indices : integer  # dimensionless index variable\n",
        "spike_count = 0 : integer\n",
        "'''\n",
        "\n",
        "# Synapse equations (Spike trace dynamics)\n",
        "\n",
        "\n",
        "syn_eqs_exc = '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iexc_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "syn_eqs_exc_output = '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iexc2_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "syn_eqs_inh= '''\n",
        "w : siemens   # Synaptic weight, conductance\n",
        "Iinh_post = w * X_pre : amp (summed)\n",
        "'''\n",
        "\n",
        "def update_batch_sum(synapses):\n",
        "    synapses.batch_sum_X += synapses.X  # Perform the operation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Helper Functions\n",
        "###############################################################################\n",
        "def make_groups(dims, eqs=eqs, with_input=False):\n",
        "    groups = []\n",
        "    for i, dim in enumerate(dims):\n",
        "        if i == 0 and with_input:\n",
        "            g = NeuronGroup(dim, input_eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "        else:\n",
        "            g = NeuronGroup(dim, eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "        g.v = EL\n",
        "        groups.append(g)\n",
        "    return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_bottom_up_connections(R, E_0, E_1):\n",
        "    S_p = Synapses(R, E_0, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_p.connect(condition='i == j')\n",
        "    S_p.w = 'w_init'\n",
        "\n",
        "    S_m = Synapses(R, E_1, model=syn_eqs_inh, on_pre='Y_post = 1*volt')\n",
        "    S_m.connect(condition='i == j')\n",
        "    S_m.w = 'w_init'\n",
        "\n",
        "    return S_p, S_m\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_top_down_connections(R, E_0, E_1):\n",
        "    S_p = Synapses(R, E_1, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_p.connect()\n",
        "    S_p.w = 'rand() * w_init'\n",
        "\n",
        "    S_m = Synapses(R, E_0, model=syn_eqs_inh, on_pre='Y_post = 1*volt')\n",
        "    S_m.connect()\n",
        "    S_m.w = 'rand() * w_init'\n",
        "\n",
        "    return S_p, S_m\n",
        "\n",
        "def make_gist_connections(Rs, G):\n",
        "    S_gist_input = Synapses(Rs[0], G, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_gist_input.connect(p=0.05)\n",
        "    S_gist_input.w = 'rand() * w_init'\n",
        "\n",
        "    # Connect G to all other layers\n",
        "    S_gist_output = Synapses(G, Rs[-1], model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_gist_output.connect(p=0.05)\n",
        "    S_gist_output.w = 'rand() * w_init'\n",
        "    return S_gist_input, S_gist_output\n",
        "\n",
        "def make_output_layer(num_classes_per_layer, max_depth, w_out_init=None):\n",
        "    if w_out_init is None:\n",
        "        w_out_init = w_init/10\n",
        "\n",
        "    total_neurons = sum([num_classes_per_layer**(d+1) for d in range(max_depth)])\n",
        "    Os = NeuronGroup(N=total_neurons, model=out_eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "\n",
        "    S_o = Synapses(Os, Os, model=syn_eqs_exc_output, on_pre='Y_post = 1*volt')\n",
        "\n",
        "    trace = 0\n",
        "    for d in range(num_classes_per_layer - 1):\n",
        "        new_trace = trace + num_classes_per_layer**(d+1)\n",
        "        source_indices = list(range(trace, trace + num_classes_per_layer**(d+1)))\n",
        "        target_indices = list(range(new_trace, new_trace + num_classes_per_layer**(d+2)))\n",
        "        ii, jj = np.meshgrid(source_indices, target_indices, indexing='ij')\n",
        "        S_o.connect(i=ii.flatten(), j=jj.flatten())\n",
        "        trace = new_trace\n",
        "\n",
        "    # Assign indices for output stimuli\n",
        "    stimulus_indices = []\n",
        "    for i in range(max_depth):\n",
        "        for _ in range(num_classes_per_layer**(i+1)):\n",
        "            stimulus_indices.append(i)\n",
        "    Os.indices = stimulus_indices\n",
        "\n",
        "    return Os, S_o\n",
        "\n",
        "def make_network(dims, num_classes_per_layer, max_depth, w_out_init=None):\n",
        "    max_depth += 1\n",
        "    Rs = make_groups(dims, with_input=True)\n",
        "    Es_0 = make_groups(dims[:-1])\n",
        "    Es_1 = make_groups(dims[:-1])\n",
        "    G = NeuronGroup(16, eqs, threshold='v > -40*mV', reset='v = Vr', method='euler')\n",
        "\n",
        "    Os, S_o_internal = make_output_layer(num_classes_per_layer, max_depth, w_out_init=w_out_init)\n",
        "\n",
        "    connections = {}\n",
        "    for i in range(len(Rs)-1):\n",
        "        S_p, S_m = make_bottom_up_connections(Rs[i], Es_0[i], Es_1[i])\n",
        "        connections[f\"bottom_up_{i}\"] = [S_p, S_m]\n",
        "\n",
        "        if i != len(Rs)-1:\n",
        "            S_p_td, S_m_td = make_top_down_connections(Rs[i+1], Es_0[i], Es_1[i])\n",
        "            connections[f\"top_down_{i}\"] = [S_p_td, S_m_td]\n",
        "\n",
        "    S_gist_input, S_gist_output = make_gist_connections(Rs, G)\n",
        "    connections[\"gist_input\"] = [S_gist_input]\n",
        "    connections[\"gist_output\"] = [S_gist_output]\n",
        "\n",
        "    # External output connections\n",
        "    # In original code: attempted to connect Rs[-1] to Os\n",
        "    # We'll do a single large synapse group for simplicity:\n",
        "    S_o_external = Synapses(Rs[-1], Os, model=syn_eqs_exc, on_pre='Y_post = 1*volt')\n",
        "    S_o_external.connect()\n",
        "    S_o_external.w = 'rand() * w_init'\n",
        "    connections[\"output_external\"] = [S_o_external]\n",
        "\n",
        "    return Rs, Es_0, Es_1, G, Os, S_o_internal, connections\n",
        "\n",
        "def normalize_tensor(tensor, old_min, old_max, new_min, new_max):\n",
        "    normalized_tensor = (tensor - old_min) / (old_max - old_min)\n",
        "    scaled_tensor = normalized_tensor * (new_max - new_min) + new_min\n",
        "    return scaled_tensor\n",
        "\n",
        "def normalize_and_unwrap_dataset(dt, minval, maxval):\n",
        "    minimum = 0\n",
        "    maximum = 0\n",
        "    all_curves = [torch.tensor(curve, dtype=torch.float64) for curve in chain.from_iterable(dt[\"curves\"].values())]\n",
        "    for curve in all_curves:\n",
        "        _min = torch.min(curve)\n",
        "        _max = torch.max(curve)\n",
        "        if (minimum > _min): minimum = _min\n",
        "        if (maximum < _max): maximum = _max\n",
        "\n",
        "    unwrapped = []\n",
        "    for key in dt[\"categories\"].keys():\n",
        "        for curve in dt[\"curves\"][key]:\n",
        "            unwrapped.append((torch.tensor(key), normalize_tensor(torch.tensor(curve), minimum, maximum, minval, maxval)))\n",
        "    return unwrapped\n",
        "\n",
        "def get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer):\n",
        "    if labels.ndim == 1:\n",
        "        labels = labels.unsqueeze(0)\n",
        "\n",
        "    indexes = torch.zeros_like(labels)\n",
        "    for i in range(labels.shape[1]):\n",
        "        summed = 0\n",
        "        if i > 0:\n",
        "            summed = torch.stack([labels[:, j]*num_classes_per_layer**(i-j) for j in range(i)]).sum(dim=0)\n",
        "        indexes[:, i] = summed + labels[:, i]\n",
        "\n",
        "    # Adjust indexes for concatenation\n",
        "    to_sum = 0\n",
        "    for i in range(labels.shape[1]):\n",
        "        if i > 0:\n",
        "            to_sum += num_classes_per_layer**i\n",
        "        indexes[:, i] = indexes[:, i] + to_sum\n",
        "    return indexes\n",
        "\n",
        "def get_output_current_arrays(I_indexes, dim, I_value):\n",
        "    if I_indexes.ndim == 1:\n",
        "        I_indexes = I_indexes.unsqueeze(0)\n",
        "\n",
        "    out = torch.zeros((I_indexes.shape[0], dim), dtype=torch.float32)\n",
        "    row_indices = torch.arange(I_indexes.shape[0]).repeat_interleave(I_indexes.shape[1])\n",
        "    col_indices = I_indexes.flatten()\n",
        "    out[row_indices, col_indices] = I_value\n",
        "    return out\n",
        "\n",
        "class CurveDataset(Dataset):\n",
        "    def __init__(self, data, minval, maxval):\n",
        "        self.data = normalize_and_unwrap_dataset(data, minval, maxval)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        labels, curves = self.data[idx]\n",
        "        return labels, curves.flatten()\n",
        "\n",
        "def collate_fn(batch, msec_step, num_pause_blocks, num_classes_per_layer, out_dim):\n",
        "    labels, curves = zip(*batch)\n",
        "    labels = torch.stack(labels, dim=0)\n",
        "    curves = torch.stack(curves, dim=0)\n",
        "\n",
        "    pause = torch.zeros_like(curves[0])\n",
        "    pause_block = torch.tile(pause, (num_pause_blocks, 1))\n",
        "    curves_with_pause = torch.vstack([torch.vstack((row, pause_block)) for row in curves])\n",
        "    visual_stimulus = TimedArray(curves_with_pause.numpy() * nA, dt=msec_step)\n",
        "\n",
        "    stimulus_indexes = TimedArray(np.tile([1] + [0] * num_pause_blocks, curves.size(0)), dt=msec_step)\n",
        "\n",
        "    out_stimuli_idx = get_output_stimuli_indexes_from_labels(labels, num_classes_per_layer)\n",
        "    out_stimuli_array = get_output_current_arrays(out_stimuli_idx, out_dim, 1)\n",
        "    label_pause = torch.zeros(out_dim)\n",
        "    label_pause_block = torch.tile(label_pause, (num_pause_blocks, 1))\n",
        "    out_stimuli_with_pause = torch.vstack([torch.vstack((row, label_pause_block)) for row in out_stimuli_array])\n",
        "    output_stimuli = TimedArray(out_stimuli_with_pause.numpy() * nA, dt=msec_step)\n",
        "\n",
        "    return stimulus_indexes, output_stimuli, visual_stimulus\n",
        "\n",
        "###############################################################################\n",
        "# Build the network and run\n",
        "###############################################################################\n",
        "dims = [2, 2, 2, 2]\n",
        "num_classes_per_layer = 3\n",
        "max_depth = 3\n",
        "\n",
        "Rs, Es_0, Es_1, G, Os, S_o_internal, connections = make_network(dims, num_classes_per_layer, max_depth)\n",
        "\n",
        "Rs[0].indices = range(Rs[0].N)\n",
        "\n",
        "su = Synthetic_Dataset_Utils()\n",
        "ranges = [30, 30, 30, 30]\n",
        "prior_params = [10, 10, 10, 10]\n",
        "num_samples_per_class = 5\n",
        "N = 15\n",
        "tree = su.build_tree(prior_params, 0, max_depth, num_classes_per_layer, 1, ranges)\n",
        "synth_dataset = su.make_dataset(tree, num_samples_per_class=num_samples_per_class, N=N)\n",
        "\n",
        "output_dim = Os.N\n",
        "lr = 0.03\n",
        "batch_size = 100\n",
        "num_pause_blocks = 1\n",
        "\n",
        "dataset = CurveDataset(synth_dataset, 0.6, 1.5)\n",
        "dl = DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=partial(\n",
        "        collate_fn,\n",
        "        msec_step=msec_step,\n",
        "        num_pause_blocks=num_pause_blocks,\n",
        "        num_classes_per_layer=num_classes_per_layer,\n",
        "        out_dim=output_dim\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "@network_operation()\n",
        "def debug():\n",
        "    print(\"Rs[0].I: \", Rs[0].I[:])\n",
        "\n",
        "@network_operation()\n",
        "def update_sums():\n",
        "    current_idx = int(defaultclock.t / msec_step)\n",
        "    if stimulus_indexes.values[current_idx] == 1:\n",
        "        for n in range(len(Rs)-1):\n",
        "            if n > 0:\n",
        "                Rs[n+1].batch_sum_X += Rs[n+1].X\n",
        "            Es_0[n].batch_sum_X += Es_0[n].X\n",
        "            Es_1[n].batch_sum_X += Es_1[n].X\n",
        "\n",
        "@network_operation()\n",
        "def apply_weight_update():\n",
        "    print(\"Applying weight update\")\n",
        "    for n in range(len(Rs)-1):\n",
        "        \n",
        "        print(\"n: \", n)\n",
        "        \n",
        "        # Retrieve top_down synapses       \n",
        "        S_p, S_m = connections[f\"top_down_{n}\"]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "            \n",
        "        for i_idx in range(Es_0[n].N):\n",
        "            for j_idx in range(Rs[n+1].N):\n",
        "                \n",
        "                S_p.w[i_idx, j_idx] += lr * Es_0[n].batch_sum_X[i_idx] * Rs[n+1].batch_sum_X[j_idx] * siemens\n",
        "                S_m.w[i_idx, j_idx] += lr * Es_1[n].batch_sum_X[i_idx] * Rs[n+1].batch_sum_X[j_idx] * siemens\n",
        "                \n",
        "     \n",
        "            # Reset after update\n",
        "            Rs[n+1].batch_sum_X[:] = 0\n",
        "            Es_0[n].batch_sum_X[:] = 0\n",
        "            Es_1[n].batch_sum_X[:] = 0\n",
        "    \n",
        "    \n",
        "    \n",
        "    external_syn = connections[\"output_external\"][0]\n",
        "    \n",
        "    for i_idx in range(Rs[-1].N):\n",
        "        for j_idx in range(Os.N):\n",
        "            \n",
        "            external_syn.w[i_idx, j_idx] += lr * Rs[-1].batch_sum_X[i_idx] * Os.batch_sum_X[j_idx] * siemens\n",
        "            \n",
        "            \n",
        "    Rs[-1].batch_sum_X[:] = 0\n",
        "    Os.batch_sum_X[:] = 0\n",
        "        \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "@network_operation()\n",
        "def log_spikes():\n",
        "    # Everything here runs on CPU\n",
        "    # Get the new spikes from spike_mon\n",
        "    # For example, we could retrieve the current length or the last spike index\n",
        "    # Then write them to a file with Python I/O\n",
        "    print(\"CHUNGUS IS BIG\")\n",
        "    new_spike_indices = spike_mon.i[:]\n",
        "    new_spike_times   = spike_mon.t[:]\n",
        "    # For demonstration, write them out\n",
        "    with open(\"/content/outputs.txt\", \"a\") as f:\n",
        "        for i_spk, t_spk in zip(new_spike_indices, new_spike_times):\n",
        "            f.write(f\"{t_spk} {i_spk}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Extract all synapses and objects for the network\n",
        "all_synapses = []\n",
        "all_synapses.append(S_o_internal)\n",
        "\n",
        "for i in range(len(Rs)-1):\n",
        "    bu_syns = connections[f\"bottom_up_{i}\"]\n",
        "    all_synapses.extend(bu_syns)\n",
        "    td_key = f\"top_down_{i}\"\n",
        "    if td_key in connections:\n",
        "        td_syns = connections[td_key]\n",
        "        all_synapses.extend(td_syns)\n",
        "\n",
        "gist_input_syn = connections[\"gist_input\"][0]\n",
        "gist_output_syn = connections[\"gist_output\"][0]\n",
        "all_synapses.append(gist_input_syn)\n",
        "all_synapses.append(gist_output_syn)\n",
        "\n",
        "external_syn = connections[\"output_external\"][0]\n",
        "all_synapses.append(external_syn)\n",
        "\n",
        "monitor = StateMonitor(Rs[0], ['v', 'I'], record=[0])\n",
        "spike_monitor = SpikeMonitor(Rs[0])\n",
        "\n",
        "s = 0\n",
        "\n",
        "\n",
        "\n",
        "spike_mon = SpikeMonitor(Os)\n",
        "\n",
        "\n",
        "net = Network(\n",
        "    Rs, Es_0, Es_1, G, Os,\n",
        "    all_synapses,\n",
        "    monitor, spike_monitor,\n",
        "    add_inputs, debug, update_sums, apply_weight_update, log_spikes\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "all_batch_rates = []\n",
        "output_spikes = []\n",
        "for s, (stimulus_indexes, output_stimuli, input_stimuli) in enumerate(dl):\n",
        "    print(f\"batch n.{s}\")\n",
        "\n",
        "      # 1) Reset the spike count to 0 if you want each batch to be measured independently\n",
        "\n",
        "    output_spikes.append(Os.spike_count)\n",
        "    # 2) Set references for the network operations\n",
        "    add_inputs.namespace['_stimulus_indexes'] = stimulus_indexes\n",
        "    add_inputs.namespace['_output_stimuli'] = output_stimuli\n",
        "    add_inputs.namespace['_input_stimuli'] = input_stimuli\n",
        "    # Run the network\n",
        "    # (batch_size + batch_size*num_pause_blocks)*100*ms\n",
        "\n",
        "    run_time = (batch_size + batch_size*num_pause_blocks)*100*ms\n",
        "    net.run(run_time)\n",
        "    #net.stop()\n",
        "\n",
        "device.build(directory='cuda_project', compile=True, run=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae2e9db",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e833076d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866a4d7d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb98e74",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86b934f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8199b6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab62e6d3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7c1a03",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3d1233",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7ea08c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f1ef15",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad61f87",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb916a1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fca1b72",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda74dff-771f-4b8e-ba4b-cb16fcafe6b2",
      "metadata": {
        "id": "dda74dff-771f-4b8e-ba4b-cb16fcafe6b2"
      },
      "outputs": [],
      "source": [
        "spike_data = spike_mon.get_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4c52dc-f78b-4f64-90d7-38ce1c545789",
      "metadata": {
        "id": "9b4c52dc-f78b-4f64-90d7-38ce1c545789"
      },
      "outputs": [],
      "source": [
        "output_spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140c791d-797d-4b45-9c58-9891429fba36",
      "metadata": {
        "id": "140c791d-797d-4b45-9c58-9891429fba36"
      },
      "outputs": [],
      "source": [
        "# Suppose you want to plot the last batch's firing rates\n",
        "all_batch_rates = [spike_monitor_os.count/run_time for spike_monitor_os in output_monitors]\n",
        "\n",
        "for batch_rates in all_batch_rates:\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.bar(range(Os.N), batch_rates)\n",
        "  plt.xlabel(\"Os Neuron Index\")\n",
        "  plt.ylabel(\"Firing rate (Hz)\")\n",
        "  plt.title(\"Average firing rate of Os neurons (last batch)\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c778742e-37c3-439d-afb7-da728be51fbf",
      "metadata": {
        "id": "c778742e-37c3-439d-afb7-da728be51fbf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d90507d-5e2d-4788-93a9-08122212ca56",
      "metadata": {
        "id": "6d90507d-5e2d-4788-93a9-08122212ca56"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28aeb00-feb8-4dea-a51d-2e9125bf402e",
      "metadata": {
        "id": "b28aeb00-feb8-4dea-a51d-2e9125bf402e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5759ba14-4035-415a-99ee-0ec09e96b446",
      "metadata": {
        "id": "5759ba14-4035-415a-99ee-0ec09e96b446"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74727b80-faea-43bb-9961-4e77ee19e496",
      "metadata": {
        "id": "74727b80-faea-43bb-9961-4e77ee19e496"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63abcfb-c9a6-4655-8f35-b03ed788c9cf",
      "metadata": {
        "id": "b63abcfb-c9a6-4655-8f35-b03ed788c9cf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a213fd1-3f27-45a1-8188-0296fe44b05c",
      "metadata": {
        "id": "2a213fd1-3f27-45a1-8188-0296fe44b05c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14832a22-ced6-42d5-a3ac-cb91bdfd7075",
      "metadata": {
        "id": "14832a22-ced6-42d5-a3ac-cb91bdfd7075"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9049ea8-a39c-4e1d-93dc-185165752230",
      "metadata": {
        "id": "b9049ea8-a39c-4e1d-93dc-185165752230"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e639ea6b-c9fa-4019-adef-734c4899d160",
      "metadata": {
        "id": "e639ea6b-c9fa-4019-adef-734c4899d160"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1dcc06-81d5-499d-9fce-eb8d7a9d089e",
      "metadata": {
        "id": "1b1dcc06-81d5-499d-9fce-eb8d7a9d089e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4485c9ac-195e-4221-a629-75af4422fcaa",
      "metadata": {
        "id": "4485c9ac-195e-4221-a629-75af4422fcaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e49ef9-29d6-40be-a0b0-defbe2f36826",
      "metadata": {
        "id": "59e49ef9-29d6-40be-a0b0-defbe2f36826"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc5af8d-ad06-45e8-a0fe-5bd678943b11",
      "metadata": {
        "id": "8cc5af8d-ad06-45e8-a0fe-5bd678943b11"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc7de56-20b5-4998-a35a-86e86f301bcd",
      "metadata": {
        "id": "8cc7de56-20b5-4998-a35a-86e86f301bcd"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(vars(spike_monitor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002d5551-26cc-4093-88ac-28068d6dec9a",
      "metadata": {
        "id": "002d5551-26cc-4093-88ac-28068d6dec9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# spike_monitor = SpikeMonitor(Os)\n",
        "\n",
        "\n",
        "# Plot membrane potential\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "\n",
        "\n",
        "# Firing rate plot\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Spike Count')\n",
        "plt.title('Firing Rate of Neuron 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zkD5Zs72axi9",
      "metadata": {
        "id": "zkD5Zs72axi9"
      },
      "outputs": [],
      "source": [
        "def training_phase_1(network, dl):\n",
        "\n",
        "  for label_input, curve_input in dl:\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lOfMXzfIaxdn",
      "metadata": {
        "id": "lOfMXzfIaxdn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T9V1Ufi3axau",
      "metadata": {
        "id": "T9V1Ufi3axau"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Membrane potential plot\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(monitor.t / ms, monitor.v[0] / mV, label='mV')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Membrane Potential (mV)')\n",
        "plt.title('Input')\n",
        "plt.legend()\n",
        "\n",
        "# Firing rate plot\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Spike Count')\n",
        "plt.title('Input')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jJZrHvJVaxXE",
      "metadata": {
        "id": "jJZrHvJVaxXE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68ee7d2-a787-41b0-a184-154cbb8d3242",
      "metadata": {
        "id": "c68ee7d2-a787-41b0-a184-154cbb8d3242"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147f51cc-b549-4b0c-83f2-eb6df4bfaf66",
      "metadata": {
        "id": "147f51cc-b549-4b0c-83f2-eb6df4bfaf66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d2226e1-6834-4dff-82e4-59e522ee9a81",
      "metadata": {
        "id": "0d2226e1-6834-4dff-82e4-59e522ee9a81"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99fb598-329c-41f4-8998-97d02000a5ea",
      "metadata": {
        "id": "c99fb598-329c-41f4-8998-97d02000a5ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719d563b-4185-43e6-b26a-41ae6cfdd1ba",
      "metadata": {
        "id": "719d563b-4185-43e6-b26a-41ae6cfdd1ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "586f1b19-bab1-4990-b2ca-1aabe4ae053c",
      "metadata": {
        "id": "586f1b19-bab1-4990-b2ca-1aabe4ae053c",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4cc4819-5071-4651-9378-4a9ab4117d33",
      "metadata": {
        "id": "c4cc4819-5071-4651-9378-4a9ab4117d33"
      },
      "outputs": [],
      "source": [
        "network = Network(\n",
        "    *Rs,        # Unpack the list of NeuronGroups\n",
        "    *Es_0,      # Unpack the list of NeuronGroups\n",
        "    *Es_1,      # Unpack the list of NeuronGroups\n",
        "    G,          # Single NeuronGroup is fine\n",
        "    Os,         # Single NeuronGroup is fine\n",
        "    connections[\"output_internal\"],   # Single Synapses object is fine\n",
        "    connections[\"gist_input\"],        # Single Synapses object is fine\n",
        "    *connections[\"gist_output\"],      # Unpack the list of Synapses\n",
        "    *connections[\"output_external\"],  # Unpack the list of Synapses\n",
        "    spike_monitor,\n",
        "    monitor,\n",
        "    debug,\n",
        "    update_sums,\n",
        "    apply_weight_update\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUtGtvxAaxRO",
      "metadata": {
        "id": "JUtGtvxAaxRO"
      },
      "outputs": [],
      "source": [
        "eqs_neuron = '''\n",
        "dv/dt = (-gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "I = stimulus(t)  : amp\n",
        "'''\n",
        "\n",
        "\n",
        "neuron = make_groups([1], eqs = eqs_neuron)[0]\n",
        "\n",
        "neuron\n",
        "\n",
        "# Change 2 to any value from 0.6 to 1.5, which is the range of current intensity we are probably going to use\n",
        "\n",
        "stimulus = TimedArray(np.hstack([[c] for c in np.ones(1)*0.8]) * nA, dt=10*ms)\n",
        "\n",
        "\n",
        "\n",
        "neuron.v = EL\n",
        "neuron.A = 0 * nA\n",
        "\n",
        "# Monitor the specific neuron (neuron 2)\n",
        "monitor = StateMonitor(neuron, ['v', 'I'], record=[0])\n",
        "spike_monitor = SpikeMonitor(neuron)\n",
        "\n",
        "run(1 * second)\n",
        "\n",
        "# Plot membrane potential\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Membrane potential plot\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(monitor.t / ms, monitor.v[0] / mV, label='Neuron 2 Membrane Potential')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Membrane Potential (mV)')\n",
        "plt.title('Stimulus Applied to Single Neuron')\n",
        "plt.legend()\n",
        "\n",
        "# Firing rate plot\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.hist(spike_monitor.t / ms, bins=10, alpha=0.7)\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Spike Count')\n",
        "plt.title('Firing Rate of Neuron 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "0.6: 0\n",
        "0.625: 0\n",
        "0.63: 0.5\n",
        "0.65: 1.5\n",
        "0.7: 2.8\n",
        "0.8: 4.5\n",
        "1: 7.5\n",
        "1.5: 14\n",
        "2: 20\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f18219-77eb-4144-8b49-d2df646ad21d",
      "metadata": {
        "id": "23f18219-77eb-4144-8b49-d2df646ad21d"
      },
      "outputs": [],
      "source": [
        "# Let's connect 2 groups as a trial\n",
        "\n",
        "G1 = NeuronGroup(5, 'v : volt', threshold='v > Vcut', reset='v = Vr', method='euler')  # Presynaptic neurons\n",
        "G2 = NeuronGroup(5, eqs, threshold='v > Vcut', reset='v = Vr; A += b', method='euler')  # Postsynaptic neurons\n",
        "\n",
        "# Initialize variables\n",
        "G1.v = EL\n",
        "G2.v = EL\n",
        "G2.A = 0 * nA\n",
        "\n",
        "# Create synapses\n",
        "S = Synapses(G1, G2, model=syn_eqs_exc,\n",
        "             on_pre='Y_post = 1*volt', method = \"euler\")  # Increment glutamate release on spike\n",
        "S.connect(p=0.1)  # Random connections\n",
        "S.w = 'rand() * w_init'  # Random initial weights\n",
        "\n",
        "# Monitors\n",
        "spike_mon_G1 = SpikeMonitor(G1)\n",
        "spike_mon_G2 = SpikeMonitor(G2)\n",
        "state_mon_G2 = StateMonitor(G2, ['v', 'I', 'A'], record=True)\n",
        "\n",
        "# Run simulation\n",
        "b2.run(500 * ms)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot membrane potential of a postsynaptic neuron\n",
        "plt.subplot(311)\n",
        "plt.plot(state_mon_G2.t / ms, state_mon_G2.v[0] / mV, label='Membrane potential (v)')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Voltage (mV)')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33487b0-f5ed-4b97-9956-8a57570ef79b",
      "metadata": {
        "id": "a33487b0-f5ed-4b97-9956-8a57570ef79b"
      },
      "outputs": [],
      "source": [
        "# TODO: Make weights always non-negative!\n",
        "\n",
        "# Parameters for neurons\n",
        "Cm = 281 * pF  # Membrane capacitance\n",
        "gL = 30 * nS   # Leak conductance\n",
        "EL = -70.6 * mV  # Leak reversal potential\n",
        "Vth = -50.4 * mV  # Spike threshold\n",
        "DeltaT = 2 * mV  # Slope factor\n",
        "Vr = -70.6 * mV  # Reset potential\n",
        "Vcut = -40 * mV  # Cutoff potential for spike generation\n",
        "tau_A = 1 * ms  # Adaptation time constant\n",
        "c = 4 * nS       # Coupling parameter\n",
        "b = 0.0805 * nA  # Spike-triggered adaptation increment\n",
        "\n",
        "# Parameters for synapses\n",
        "tau_rise = 5 * ms  # Rise time constant for AMPA\n",
        "tau_decay = 50 * ms  # Decay time constant for NMDA\n",
        "w_init = 0.5 * nS   # Initial synaptic weight (conductance)\n",
        "\n",
        "# AdEx neuron equations\n",
        "eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "I : amp  # Input current\n",
        "'''\n",
        "\n",
        "input_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "I = input_stimuli(t)  : amp\n",
        "'''\n",
        "\n",
        "out_eqs = '''\n",
        "dv/dt = ( -gL * (v - EL) + gL * DeltaT * exp((v - Vth) / DeltaT) + I_tot - A ) / Cm : volt\n",
        "dA/dt = (c * (v - EL) - A) / tau_A : amp\n",
        "I : amp  # Synaptic input (summed)\n",
        "I_tot = I + output_stimuli(t, indices) : amp\n",
        "indices : integer  # dimensionless index variable\n",
        "'''\n",
        "\n",
        "# Synapse equations (Spike trace dynamics)\n",
        "\n",
        "\n",
        "syn_eqs_exc = '''\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt   # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "w : siemens  # Synaptic weight (conductance)\n",
        "I_post = w * X : amp (summed)\n",
        "batch_sum_X : volt  # Accumulate X values over a batch\n",
        "running_sum_X : volt  # Sum of X during a stimulus\n",
        "'''\n",
        "\n",
        "syn_eqs_inh= '''\n",
        "dX/dt = -X / tau_decay + Y / tau_rise : volt  # Spike trace\n",
        "dY/dt = -Y / tau_decay : volt  # Glutamate decay\n",
        "w : siemens  # Synaptic weight (conductance)\n",
        "I_post = w * X : amp (summed)\n",
        "batch_sum_X : volt  # Accumulate X values over a batch\n",
        "running_sum_X : volt  # Sum of X during a stimulus\n",
        "'''\n",
        "\n",
        "def update_batch_sum(synapses):\n",
        "    synapses.batch_sum_X += synapses.X  # Perform the operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f595c7b-4896-4d7e-8dfb-3590cd46a6c0",
      "metadata": {
        "id": "0f595c7b-4896-4d7e-8dfb-3590cd46a6c0"
      },
      "outputs": [],
      "source": [
        "# Update logic\n",
        "@network_operation(dt = batch_size * msec_step * ms)\n",
        "def update_sums( ):\n",
        "\n",
        "    current_idx = int(defaultclock.t / msec_step*ms)\n",
        "\n",
        "    if stimulus_indexes.values[current_idx] == 1:\n",
        "\n",
        "\n",
        "        for n in range(len(Rs)-1):\n",
        "\n",
        "            for i in range(len(Rs[n])):\n",
        "\n",
        "                '''\n",
        "                Rs[n].batch_sum_X[i] += Rs[n].X_[i]\n",
        "\n",
        "                Es_0[n].batch_sum_X[i] += Es_0[n].X_[i]\n",
        "\n",
        "                Es_1[n].batch_sum_X[i] += Es_1[n].X_[i]\n",
        "                '''\n",
        "\n",
        "                Rs[n].batch_sum_X += Rs[n].X_\n",
        "                Es_0[n].batch_sum_X += Es_0[n].X_\n",
        "                Es_1[n].batch_sum_X += Es_1[n].X_\n",
        "\n",
        "\n",
        "@network_operation( dt = batch_size * msec_step * ms )\n",
        "def apply_weight_update():\n",
        "\n",
        "    print(\"Applying weight update\")\n",
        "\n",
        "    for n in range(len(Rs)-1):\n",
        "        print(\"n: \", n)\n",
        "\n",
        "        for i in range(len(Rs[n])):\n",
        "            print(\"i: \", i)\n",
        "            for j in range(len(Es_0[n])):\n",
        "                print(\"j: \", j)\n",
        "\n",
        "\n",
        "                S_p, S_m = connections[f\"top_down_{n}\"]\n",
        "\n",
        "                S_p.w[i,j] += lr * Es_0[n].batch_sum_X[j] * Rs[n].batch_sum_X[i] * siemens\n",
        "\n",
        "                S_m.w[i,j] += lr * Es_1[n].batch_sum_X[j] * Rs[n].batch_sum_X[i]  * siemens\n",
        "\n",
        "                Rs[n].batch_sum_X[i] = 0\n",
        "\n",
        "                Es_0[n].batch_sum_X[i]= 0\n",
        "\n",
        "                Es_1[n].batch_sum_X[i] = 0\n",
        "\n",
        "\n",
        "\n",
        "for stimulus_indexes, output_stimuli, input_stimuli in dl:\n",
        "\n",
        "    print(\"indexes\", stimulus_indexes.values.shape, stimulus_indexes.values)\n",
        "\n",
        "    run((batch_size + batch_size*num_pause_blocks) * 100 * ms)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
